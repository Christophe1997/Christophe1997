<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on CastOff</title>
    <link>https://christophe1997.github.io/categories/notes/</link>
    <description>Recent content in Notes on CastOff</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 25 Feb 2020 11:50:04 +0800</lastBuildDate><atom:link href="https://christophe1997.github.io/categories/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Bitwise Tricks</title>
      <link>https://christophe1997.github.io/posts/2020/some-bitwise-tricks/</link>
      <pubDate>Tue, 25 Feb 2020 11:50:04 +0800</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2020/some-bitwise-tricks/</guid>
      <description>Bitwise tricks 近来翻到一本&amp;quot;Hackers Delight&amp;quot;的书, 其主要介绍基于二进制运算的算法. 初读来大感震撼, 其结果之巧妙,过程之精简, 于仅仅是了解计算机数是用补码所表示, 并未深入了解过二进制运算的人而言不可谓不精美. 正如Dijkstra所言, 计算机程序是集逻辑美感与机械实现的矛盾体. 本文姑且将其中的一些皮毛摘录如下, 以便日后之使用, 目前倒是难以得到应用.
  使最右位的1变成0
1 2 3  // 01011000 -&amp;gt; 01010000 // if no one then return 0 x &amp;amp; (x - 1)   上式也可以用来判断一个无符号的整数是否是$ 2^n $的形式.
  使最右位的0变成1
1 2 3  // 10100111 -&amp;gt; 10101111 // if no zero then return -1 x | (x + 1)     使尾部的1变成0</description>
    </item>
    
    <item>
      <title>Object Lifetime and Storage Management</title>
      <link>https://christophe1997.github.io/posts/2018/object-lifetime-and-storage-management/</link>
      <pubDate>Sat, 08 Dec 2018 16:48:49 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/object-lifetime-and-storage-management/</guid>
      <description>在考虑标识符和绑定(bindings)的时候, 关键在于区分标识符和它们所引用的对象, 以及以下事件:
 对象的创建 绑定的创建 所有使用绑定的情况, 诸如引用变量, 子程序, 类型等 停用和重用那些暂时没有用的绑定 绑定的析构(destruction) 对象的析构  绑定的生命周期指的是这个绑定从创建到析构的整个过程, 类似的可以定义对象的生命周期. 通常情况下, 一个对象的生命周期可能会大于对应的绑定的生命周期, 即当 标识符不再引用该对象时, 该对象依然存在(例如在子程序中传入某个对象的引用, 如C++中的&amp;amp;参数). 当然, 一个绑定的生命周期也有可能大于对应对象的生命周期, 虽然这通常被认为是一个BUG.
对象的生命周期通常与以下内存分配(storage allocation)机制有关:
 静态(static)对象会在程序的整个运行过程中被分配一个绝对地址. 栈(stack)对象随着子程序的调用和返回而被创建以及按照LIFO的顺序析构. 堆(heap)对象可以在任何时候创建和析构, 其额外要求更加通用和昂贵的内存分配算法.  静态分配(static allocation) 静态分配最明显的例子就是全局对象, 当然全局对象不是唯一的例子. 构成程序机器语言翻译的指令也可以认为是静态分配的变量; 数字和字符串的常量当然也是静态分配 的; 另外很多编译器会产生一系列的表用于支持运行时的debug, 动态类型检查, 垃圾回收, 异常处理等, 这些表也都是静态分配的. 静态分配的对象通常希望它们的值 不在变化, 因此经常被分配在被保护的只读的内存中以方便在试图修改其值产生中断并抛出运行时错误.
在很多语言中,一个具名常量通常要求有一个能够在编译期确定的初始值. 通常这些初始值都被限制在那些已知的常量以及内置的函数. 这些具名常量加上字面常量通常被 叫做表现常量(manifest constants)或者编译期常量(compile-time constants). 在某些语言中(C, Ada), 常量仅仅只是那些无法在elaboration time之后改变的值, 这些值可能依赖与其他在运行时才能确定的值. 这些elaboration-time的常量在作为递归函数的局部变量时必须要分配在栈上. C#显示提供了 声明两种常量的方法, 即const和readonly关键字.
另外编译器通常对于子程序的某些值采用特定的分配策略:
 参数和返回值, 编译器通常会尽可能的将这些值存在寄存器中. 临时变量, 通常是那些复杂计算过程的中间值, 一个优秀的编译器也会将它们保存在寄存器内. bookkeeping information, 这些通常包含子程序的返回地址.</description>
    </item>
    
    <item>
      <title>进程</title>
      <link>https://christophe1997.github.io/posts/2018/process/</link>
      <pubDate>Fri, 03 Aug 2018 13:10:58 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/process/</guid>
      <description>The abstraction provided by the OS of a running program is something we call a process. And there are some APIs must be included in any interface of an operating system:
 create destroy wait miscellaneous control, most OS provide some kind of method to suspend a process and resume it. status, there are usually interfaces to get some status information about a process.  The first thing that the OS must do to run a program is to load its code and any static data into memory, into the address space of the process.</description>
    </item>
    
    <item>
      <title>Functional Data Structure 2</title>
      <link>https://christophe1997.github.io/posts/2018/functional-data-structure-2/</link>
      <pubDate>Tue, 24 Jul 2018 15:54:24 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/functional-data-structure-2/</guid>
      <description>Amortization Implementations with good amortized bounds are often simpler and faster than implementations with comparable worst-case bounds. Given a sequence of operations, we may wish to know the running time of the entire sequence, but not care about the running time of any individual operation.
For instance, given a sequence of n operations, we may wish to bound the total running time of the sequence by O(n) without insisting than every individual operation run in O(1) time.</description>
    </item>
    
    <item>
      <title>Functional Data Structure 1</title>
      <link>https://christophe1997.github.io/posts/2018/functional-data-structure-1/</link>
      <pubDate>Thu, 19 Jul 2018 17:00:11 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/functional-data-structure-1/</guid>
      <description>Introduction To implement data structure in a functional way, there are two basic problems. First, from the point of view of designing and implementing efficient data structures, functional programming&amp;rsquo;s stricture against destructive updates(i.e. assignments) is a staggering handicap, tantamount to confiscating a master chef&amp;rsquo;s knives.
Imperative data structures often rely on assignments in crucial ways, and so different solutions must be found for functional programs. The second difficulty is that functional data strcutures are expected to be more flexible than their imperative counterparts.</description>
    </item>
    
    <item>
      <title>Address Space</title>
      <link>https://christophe1997.github.io/posts/2018/address-space/</link>
      <pubDate>Sun, 24 Jun 2018 23:54:04 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/address-space/</guid>
      <description>The Address Space The address space is the abstraction that OS is providing to the running program. The address space of a process contains all of the memory state of the running program. For example, the code of the program, the stack and the heap. In the sight of the program, it loaded into at a particular address and has a potentially very large address space, thus, we say that the OS is virtualizing memory.</description>
    </item>
    
    <item>
      <title>Computer System 4</title>
      <link>https://christophe1997.github.io/posts/2018/computer-system-4/</link>
      <pubDate>Sun, 10 Jun 2018 19:14:22 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/computer-system-4/</guid>
      <description>Linking Linking is the process of collecting and combining various pieces of code and data into a single file that can be loaded (copied) into memory and executed. Linking can be performed at compile time, when the source code is translated into machine code; at load time, when the program is loaded into memory and executed by the loader; and even at run time, by application programs. On modern systems, linking is performed automatically by programs called linkers.</description>
    </item>
    
    <item>
      <title>Computer System 3</title>
      <link>https://christophe1997.github.io/posts/2018/computer-system-3/</link>
      <pubDate>Fri, 01 Jun 2018 23:23:16 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/computer-system-3/</guid>
      <description>Optimization Writing an efficient program requires several type of activities. First, we must select an appropriate set of algorithms and data structures. Second, we must write source code that the compiler can effectively optimize to turn into efficient executable code. A third technique for dealing with especially demanding computations is to divide a task into portions that can be computed in parallel, on some combination of multiple cores and multiple processors.</description>
    </item>
    
    <item>
      <title>Computer System 2</title>
      <link>https://christophe1997.github.io/posts/2018/computer-system-2/</link>
      <pubDate>Mon, 28 May 2018 17:20:36 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/computer-system-2/</guid>
      <description>Assembler Computer execute machine code, sequences of bytes encoding the low-level operations that manipulate data manage memory, read and write data on storage devices, and communicate over networks.
We will focus on x86-64, the commonest machine language used in processor with laptop and PC, also it&amp;rsquo;s widely used in supercomputer and lager data center.
the x86-64 machine code is much different with the corresponding C code, the processor states below are everywhere:</description>
    </item>
    
    <item>
      <title>Computer System 1</title>
      <link>https://christophe1997.github.io/posts/2018/computer-system-1/</link>
      <pubDate>Wed, 23 May 2018 23:54:04 +0000</pubDate>
      
      <guid>https://christophe1997.github.io/posts/2018/computer-system-1/</guid>
      <description>Hardware Organization of A System A typical system has those hadrware below:
  Buses: Running throughout the system is a collection of electrical conduits called buses that carry bytes of ingormation back and forth betweent the components. Buses are typically designed to transfer fiexed sized chunks of bytes know as words. The number of bytes in a word is a fundamental system parameter that varies across systems. Most have word sizes of 8 bytes (64 bits)today</description>
    </item>
    
  </channel>
</rss>
